# Summary

## Breakdown
* Part 1 - Data preprocessing - Completed on 04-07-2020
* Part 2 - Regression - Completed on 06-08-2020
    * Simple Linear regression - Completed on 06-07-2020
    * Multiple Linear regression - Completed on 20-07-2020
    * Polynomial Regression - Completed on 23-07-2020
    * Support Vector Regression - Completed on 25-07-2020
    * Decision Tree Regression - Completed on 27-07-2020 
    * Random forest Regression - Completed on 29-07-2020
    * Evaluating Regression Models performance - Completed on 04-08-2020
    * Choosing a regression model - Completed on 06-08-2020
* Part 3 - Classification - Completed on 04-09-2020
    * Logistic Regression - Completed on 08-08-2020
    * K Nearest Neighbours - Completed on 09-08-2020 
    * SVM - Completed on 17-08-2020
    * Kernel SVM - Completed on 20-08-2020
    * Naive Bayes - Completed on 23-08-2020
    * Decision Trees - Completed on 25-08-2020
    * Random Forest - Completed on 28-08-2020
    * Evaluating classification models performance- Completed on 03-09-2020
    * Classification Model template performance comparison - Completed on 04-09-2020
* Part 4 - Clustering - Completed on - 10-09-2020
    * K - Means -Clustering - Completed on - 09-09-2020
    * Heirarchical Clustering - Completed on - 10-09-2020
* Part 5 - Association Rule Learning- 14-09-2020
    * Apriori - Completed on 13-09-2020
    * Eclat - Completed on 14-09-2020
* Part 6 - Reinforcement Learning - Completed on 19-09-2020
    * Upper Confidence Bound - Completed on 17-09-2020
    * Thompson Sampling - Completed on 19-09-2020
* Part 7 - Natural Language Processing - Completed on 24-09-2020
    * Bag of words model - Completed on 24-09-2020
* Part 8 - Deep Learning - Currently Pursuing
    * Artificial Neural Network - Currently Pursuing

## Doubts
* Splitting data into test and training sets -> Why not CV as per andrew NG?
* Why using same scaler instance on both training and test set in feature scaling? -> Asked on 04-07-2020
* Assumptions of linear regression -> Linearity , Homoscedasticity, Multivariate normality, Independance of errors,Lack of multicollinearity
* Question on instance unsolved.
* 

## Important Links
* [Datasets and code google drive link](https://drive.google.com/drive/folders/1OFNnrHRZPZ3unWdErjLHod8Ibv2FfG1d)
* [Classification / Regression templates for performance evaluation / model selection](https://drive.google.com/drive/folders/1O8vabaxga3ITjCWfwD79Xnyf8RavYuyk)
* [M L Summary and FAQ PDF](./material/Machine_Learning_A_Z_Q_A.pdf)
* [Regression Models Pros / Cons](./material/Classification_Pros_Cons.pdf)
* [Classification Models Pros / Cons](./material/Classification_Pros_Cons.pdf)
* [Upper confidence bound algorithm](./material/UCB_Algorithm_Slide.png)
* [Thompson sampling algorithm](./material/Thompson_Sampling_Slide.png)

## Todo:
* [Multi linear Regression Backward elimination bonus -1](https://www.dropbox.com/sh/pknk0g9yu4z06u7/AADSTzieYEMfs1HHxKHt9j1ba?dl=0)
* [Multi linear regression Bonus Exercise 2](https://www.superdatascience.com/pages/ml-regression-bonus-2)
* Least Square Difference and Support Vector Regression reading
* Least Square - Find literature
* [SVR  - additional reading](https://core.ac.uk/download/pdf/81523322.pdf)
* R Square adjusted - notes
* [Blog for understanding regression and classification](https://www.superdatascience.com/blogs/the-ultimate-guide-to-regression-classification)
* [Practical case study - Logistic Regression](https://www.udemy.com/course/logistic-regression-cancer-detection-case-study/?referralCode=7E62BC258B645C95D9F5)
* [Types of Kernels - Kernel SVMS](http://crsouza.com/2010/03/17/kernel-functions-for-machine-learning-applications/)
* [Types of Kernels - Kernel SVMS](https://datafreakankur.com/machine-learning-kernel-functions-3d-visualization/)
* CAP curve and CAP curve analysis notes
* [Classification Pros Cons](./material/Classification_Pros_Cons.pdf)
* [Clustering Pros Cons](./material/Clustering-Pros-Cons.pdf)
* Upper confidence bound - Regret whitepaper - Austria
* [NLP with BERT guide](https://sdsclub.com/bert-google-nlp-algorithm/)
* [NLP with BERT course](https://www.udemy.com/course/natural-language-processing-with-bert/learn/lecture/18889310#overview)
* [Homework challenge NLP](https://www.udemy.com/course/machinelearning/learn/lecture/6085634#overview)
* [Efficient BackProp by Yann LeCun 1998](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf)
* [Deep sparse rectifier neural networks](http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf)

> .ipynb python notebooks can be opened using Anaconda IDE and Jupyter / Spyder notebooks or google colab.