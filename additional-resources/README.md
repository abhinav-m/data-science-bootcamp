# Additional resources

> A list of additional resources / references and todos



## Todo:

### Udemy (Super data science is by same authors):
* [Multi linear Regression Backward elimination bonus -1](https://www.dropbox.com/sh/pknk0g9yu4z06u7/AADSTzieYEMfs1HHxKHt9j1ba?dl=0)
* [Multi linear regression Bonus Exercise 2](https://www.superdatascience.com/pages/ml-regression-bonus-2)
* [Blog for understanding regression and classification](https://www.superdatascience.com/blogs/the-ultimate-guide-to-regression-classification) - Done on 05-10-2020
* [Practical case study - Logistic Regression](https://www.udemy.com/course/logistic-regression-cancer-detection-case-study/?referralCode=7E62BC258B645C95D9F5)
* [Classification Pros Cons](./material/Classification_Pros_Cons.pdf) //FIXME
* [Clustering Pros Cons](./material/Clustering-Pros-Cons.pdf)
* [NLP with BERT course](https://www.udemy.com/course/natural-language-processing-with-bert/learn/lecture/18889310#overview)
* [Homework challenge NLP](https://www.udemy.com/course/machinelearning/learn/lecture/6085634#overview)
* [Ultimate guide to ANN](https://www.superdatascience.com/blogs/the-ultimate-guide-to-artificial-neural-networks-ann)
* [ANN for regression free course](https://www.udemy.com/cart/subscribe/course/2968824/)
* [CNN - Bonus Resources](https://www.superdatascience.com/blogs/the-ultimate-guide-to-convolutional-neural-networks-cnn)
* [LDA explanation](https://www.udemy.com/course/machinelearning/learn/lecture/20141692#overview)
* [Cat boost](https://www.superdatascience.com/pages/ml-model-selection-bonus)


## Blogs / Books:
* [SVR  - additional reading](https://core.ac.uk/download/pdf/81523322.pdf)
* [Kernel codes in C#](https://github.com/accord-net/framework/tree/development/Sources/Accord.Statistics/Kernels)
* [Types of Kernels - Kernel SVMS](http://crsouza.com/2010/03/17/kernel-functions-for-machine-learning-applications/) -> Currently pursuing
* [Types of Kernels - Kernel SVMS](https://datafreakankur.com/machine-learning-kernel-functions-3d-visualization/) -> Completed on 06-10-2020
* [NLP with BERT guide](https://sdsclub.com/bert-google-nlp-algorithm/)
* [Efficient BackProp by Yann LeCun 1998](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf)
* [Deep sparse rectifier neural networks](http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf)
* [Neural network in 13 lines of python](https://iamtrask.github.io/2015/07/27/python-network-part2/)
* [Cost function in neural networks along with their applications](https://stats.stackexchange.com/questions/154879/a-list-of-cost-functions-used-in-neural-networks-alongside-applications)
* [Neural networks and deep learning - how gradient descent works](http://neuralnetworksanddeeplearning.com/chap2.html) 
* [Gradient based learning applied to document recognition - Yann Lecun](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.138.1115)
* [Introduction to CNN](https://cs.nju.edu.cn/wujx/teaching/15_CNN.pdf)
* [CNN](https://cs.nju.edu.cn/wujx/)
* [Gimp Conv Matrix](docs.gimp.org/en/plug-in-convmatrix.html)
* [Understanding CNN with mathematical models](https://arxiv.org/abs/1609.04112)
* [Delving Deep into rectifiers](https://arxiv.org/abs/1502.01852)
* [Evaluation of Pooling operations](http://ais.uni-bonn.de/papers/icann2010_maxpool.pdf)
* [Deep learning papers](https://adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html)
* [A friendly introduction to Cross-Entropy Loss](https://rdipietro.github.io/friendly-intro-to-cross-entropy-loss/)
* [PCA visualization explanation](https://setosa.io/ev/principal-component-analysis/) -> Completed on 06-10-2020
* [MIT opencourseware least squares](https://www.youtube.com/watch?v=YwZYSTQs-Hk) -> Completed on 05-10-2020
* [Least Squares explanation](https://docs.google.com/document/d/1vXgizn0Zz5VM_mTEfrRqwydfCyn0OY5DNlkieFLRU68/edit) -> Completed on 05-10-2020
* [Stats through visualization](https://setosa.io/ev/)
* [Time series analysis](https://towardsdatascience.com/the-complete-guide-to-time-series-analysis-and-forecasting-70d476bfe775) -> Currently reviewing

## To find:
* Upper confidence bound - Regret whitepaper - Austria
* Applying dimensionality reduction on sample datasets
* Least Square Difference and Support Vector Regression reading
* Least Square - Find literature
* R Square adjusted - notes
* CAP curve and CAP curve analysis notes
