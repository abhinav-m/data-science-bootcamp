# Large scale machine learning
 > ML algorithms have seen a significant improvement now from a few years ago due to the large collection of datasets available now. Often it is not about the algorithm of the problem, but who has more data to solve the problem.

# Key takeaways:
 * Taking the example of gradient descent, if we have say 100 million data set, computing the summation of all 100 million training examples to calculate the gradient would be very computationally inefficient, and would take a lot of time. 

 * This can be overcome by taking a subset of these examples, say 1000 and training / computing on these before designing a large scale machine learning system

* Plotting learning curves, to check whether examples have high bias / low variance can be useful to further check on algorithms computation.
  
* Large scale machine learning examples need computationally effective methods to be learnt.

[Large scale machine learning](https://www.coursera.org/learn/machine-learning/lecture/CipHf/learning-with-large-datasets)